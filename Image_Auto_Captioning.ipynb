{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The notebook consists of code to all the experiments performed, with all the CNN architectures mentioned in the report getting used. The preprocessing code of both the images as well as the annotations along with the feature generator input to the LSTM and the caption generator code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import matplotlib as matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "from PIL import Image\n",
    "import glob\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model\n",
    "from time import time\n",
    "import pickle\n",
    "from pickle import dump, load\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras import Input, layers\n",
    "from keras.preprocessing import sequence\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import pydotplus\n",
    "import pydot\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.merge import add\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from datetime import datetime\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications import nasnet\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the captions\n",
    "file = r'flickr8kcaptions.txt'\n",
    "file = open(file, 'r')\n",
    "doc = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the ids\n",
    "train_dataid = 'train_dataid.txt'\n",
    "file2 = open(train_dataid, 'r')\n",
    "doc2 = file2.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary where key is the image ID and its values is an array of its five captions\n",
    "description = dict()\n",
    "desc = []   \n",
    "imageID =[]\n",
    "lines = doc.split('\\n')\n",
    "#lines = lines[1:]\n",
    "\n",
    "for line in lines:\n",
    "    if line:\n",
    "        token = line.split('\\t')\n",
    "        imageId,desc = token[0],token[1]\n",
    "        imageId = imageId.split('.')[0]\n",
    "        if imageId not in description:\n",
    "            description[imageId] = list()\n",
    "            description[imageId].append(desc)\n",
    "        else:\n",
    "            description[imageId].append(desc)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the annotations data for whole dataset.\n",
    "table = str.maketrans('','', string.punctuation)\n",
    "for key, desc_list in description.items():\n",
    "    for i in range(len(desc_list)):\n",
    "        desc = desc_list[i]\n",
    "        desc = desc.split()\n",
    "        desc = [word.lower() for word in desc ]\n",
    "        desc = [w.translate(table) for w in desc]\n",
    "        desc = [word for word in desc if len(word)>1]\n",
    "        desc = [word for word in desc if word.isalpha()]\n",
    "        desc_list[i] = ' '.join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking out captions for training set\n",
    "content = doc2.split('\\n')\n",
    "tdict = dict()\n",
    "train_imageid = []\n",
    "for ID in content:\n",
    "    if ID:\n",
    "        train_imageid.append(ID.split('.')[0])\n",
    "for img in train_imageid:\n",
    "    if img in description:\n",
    "        tdict[img] = []\n",
    "        for desc in description[img]:\n",
    "            tdict[img].append('startseq ' + desc + ' endseq')\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "tdict['1003163366_44323f5815']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "for key in description.keys():\n",
    "    [vocabulary.update(d.split()) for d in description[key]]\n",
    "print('original vocabulary size is %d' %len(vocabulary))\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking out vocabulary words that are very less frequent\n",
    "\n",
    "all_captions = []\n",
    "\n",
    "for key, val in description.items():\n",
    "    for captions in val:\n",
    "        all_captions.append(captions)\n",
    "\n",
    "\n",
    "threshold = 10\n",
    "word_count = {}\n",
    "nsents = 0\n",
    "\n",
    "for sent in all_captions:\n",
    "    nsents  +=1\n",
    "    for w in sent.split(' '):\n",
    "        word_count[w] = word_count.get(w, 0) + 1\n",
    "\n",
    "vocab  = [word for word in word_count if word_count[word] >= threshold]\n",
    "len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads train images IDs.\n",
    "\n",
    "\n",
    "train_id =[]\n",
    "trainid_file = 'train_dataid.txt'\n",
    "doc = open(trainid_file, 'r')\n",
    "img_id = doc.read()\n",
    "img_id = img_id.split('\\n')\n",
    "\n",
    "for Id in img_id:\n",
    "    if Id:\n",
    "        sep = Id.split('.')[0]\n",
    "        train_id.append(sep)\n",
    "print('dataset: %d' %len(train_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valdict={}\n",
    "val_id = []\n",
    "val_file = 'val_dataid.txt'\n",
    "val_doc = open(val_file,'r')\n",
    "val_data = val_doc.read()\n",
    "val_data = val_data.split('\\n')\n",
    "\n",
    "for Id in val_data:\n",
    "    if Id:\n",
    "        sep = Id.split('.')[0]\n",
    "        val_id.append(sep)\n",
    "for img in val_id:\n",
    "    if img in description:\n",
    "        valdict[img] = []\n",
    "        for desc in description[img]:\n",
    "            valdict[img].append('startseq ' + desc + ' endseq')\n",
    "print('dataset: %d' %len(val_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading images\n",
    "\n",
    "images = 'Images/Images/'\n",
    "\n",
    "img = glob.glob(images + '*.jpg')\n",
    "print(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images to be used in training the models\n",
    "\n",
    "train_images = set(open('train_dataid.txt', 'r').read().strip().split('\\n'))\n",
    "\n",
    "train_img = []\n",
    "\n",
    "for i in img:\n",
    "    j = i.split('/')[-1]\n",
    "    if j in train_images:\n",
    "        train_img.append(i)\n",
    "\n",
    "print(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Images of validation data\n",
    "\n",
    "val_path = 'val_dataid.txt'\n",
    "\n",
    "val_images = set(open(val_path, 'r').read().strip().split('\\n'))\n",
    "val_img = []\n",
    "\n",
    "for i in img:\n",
    "    j = i.split('/')[-1]\n",
    "    if j in val_images:\n",
    "        val_img.append(i)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images of the test data\n",
    "\n",
    "test_path = 'test_dataid.txt'\n",
    "\n",
    "\n",
    "test_images = set(open(test_path, 'r').read().strip().split('\\n'))\n",
    "test_img = []\n",
    "\n",
    "for i in img:\n",
    "    j = i.split('/')[-1]\n",
    "    if j in test_images:\n",
    "        test_img.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    all_images = image.load_img(img, target_size=(299,299))\n",
    "    x_axis = image.img_to_array(all_images)\n",
    "    print(x_axis.shape)\n",
    "    x_axis = np.expand_dims(x_axis, axis = 0)\n",
    "    print(x_axis.shape)\n",
    "    x_axis = preprocess_input(x_axis)\n",
    "    return x_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir('Images/Images'):\n",
    "    all_images = image.load_img(f'Images/Images/{f}', target_size=(299,299))\n",
    "    x_axis = image.img_to_array(all_images)\n",
    "    print(x_axis.shape)\n",
    "    x_axis = np.expand_dims(x_axis, axis = 0)\n",
    "    print(x_axis.shape)\n",
    "    x_axis = preprocess_input(x_axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess all images\n",
    "trial_img = image.load_img('pic_screen.png', target_size = (299, 299))\n",
    "x = image.img_to_array(trial_img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INCEPTION V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancing the inceptionv3 model and using the trained model on imagenet dataset\n",
    "\n",
    "model = InceptionV3(weights = 'imagenet' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the softmax layer from the neural network\n",
    "\n",
    "cnn = Model(model.input, model.layers[-2].output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filter(layer, x,y):\n",
    "    filters = layer.get_weights()\n",
    "    fig = plt.figure()\n",
    "    for i in range(len(filters)):\n",
    "        ax = fig.add_subplot(y,x, i+1)\n",
    "        ax.matshow(filters[i][0], cmap = matplotlib.cm.binary)\n",
    "        plt.xticks(np.array([]))\n",
    "        plt.yticks(np.array([]))\n",
    "    plt.tight_layout()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To encode an image to a feature vector of (2048,)\n",
    "\n",
    "def vector(img):\n",
    "    img = preprocess(img)\n",
    "    featvec = cnn.predict(img)\n",
    "    featvec = np.reshape(featvec, featvec.shape[1])\n",
    "    return featvec\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding all images\n",
    "\n",
    "check = time()\n",
    "enc_train = {}\n",
    "\n",
    "for img in train_img:\n",
    "    enc_train[img[len(images):]] = vector(img)\n",
    "print('time taken in sec', time()-check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the features\n",
    "with open('enc_trainimg.pkl', 'wb') as enc_pkl:\n",
    "    pickle.dump(enc_train, enc_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding all images\n",
    "\n",
    "check = time()\n",
    "enc_val = {}\n",
    "\n",
    "for img in val_img:\n",
    "    enc_val[img[len(images):]] = vector(img)\n",
    "print('time taken in sec', time()-check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the features\n",
    "#with open('enc_valimg.pkl', 'wb') as enc_valpkl:\n",
    "#    pickle.dump(enc_val, enc_valpkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding all test images\n",
    "check = time()\n",
    "enc_test = dict()\n",
    "\n",
    "for img in test_img:\n",
    "    enc_test[img[len(images):]] = vector(img)\n",
    "print('time taken in sec', time()-check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_vector = vector('pic_screen.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualising CNN layers\n",
    "\n",
    "layer_names = []\n",
    "\n",
    "for layer in cnn.layers[:8]:\n",
    "    layer_names.append(layer.name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving test feature\n",
    "\n",
    "with open('enc_testing.pkl', 'wb') as enc_pickle:\n",
    "    pickle.dump(enc_test, enc_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = load(open('enc_trainimg.pkl','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = load(open('enc_valimg.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering all the train captions\n",
    "\n",
    "all_traincap =[]\n",
    "\n",
    "for key, val in tdict.items():\n",
    "    for cap in val:\n",
    "        all_traincap.append(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrd = [i.split() for i in all_traincap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = []\n",
    "\n",
    "for i in wrd:\n",
    "    unique.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = list(set(unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unique.pkl', 'wb') as pickle_d:\n",
    "    pickle.dump(unique, pickle_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = pickle.load(open('unique.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtoix2 = {val:index for index, val in enumerate(unique)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtoix2['startseq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ixtoword2 = {index:val for index, val in enumerate(unique)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ixtoword2[1847]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating word and its index\n",
    "\n",
    "wordtoix = {}\n",
    "ixtoword ={}\n",
    "\n",
    "ix = 1\n",
    "for word in vocab:\n",
    "    wordtoix[word] = ix\n",
    "    ixtoword[ix] = word\n",
    "    ix +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(ixtoword) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking out all the captions\n",
    "\n",
    "def app_cap(desc):\n",
    "    tot_cap =[]\n",
    "    for key, val in description.items():\n",
    "        for cap in val:\n",
    "            tot_cap.append(cap)\n",
    "    return tot_cap\n",
    "\n",
    "# Finding max length of captions\n",
    "\n",
    "\n",
    "def max_length(captions):\n",
    "    sen = app_cap(captions)\n",
    "    return max(len(line.split()) for line in sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max_length(tdict)\n",
    "print('The max length of caption is: %d ' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(description, pics, wordtoix, max_length, num_photos_per_batch):\n",
    "    X1, X2, Y = list(), list(), list()\n",
    "    n = 0\n",
    "    while 1:\n",
    "        for key, list_desc in description.items():\n",
    "            n+=1\n",
    "            photos = pics[key + '.jpg']\n",
    "            for desc in list_desc:\n",
    "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen = max_length)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes= vocab_size)[0]\n",
    "                    X1.append(photos)\n",
    "                    X2.append(in_seq)\n",
    "                    Y.append(out_seq)\n",
    "            if n==num_photos_per_batch:\n",
    "                yield [[array(X1), array(X2)], array(Y)]\n",
    "                X1, X2, Y = list(), list(), list()\n",
    "                n=0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = data_generator(tdict, train_features, wordtoix, max_length, number_photo_per_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a pretrained Glove model \n",
    "\n",
    "glove_dir = 'Glove200'\n",
    "\n",
    "embedding_index = {}\n",
    "\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding = 'utf-8') \n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coef = np.asarray(values[1:], dtype = 'float32')\n",
    "    embedding_index[word] = coef\n",
    "f.close()\n",
    "\n",
    "print(len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 200\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, emb_dim))\n",
    "\n",
    "for word, i in wordtoix.items():\n",
    "    emb_vector = embedding_index.get(word)\n",
    "    if emb_vector is not None:\n",
    "        embedding_matrix[i] = emb_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 1\n",
    "\n",
    "inputs1 = Input(shape = (2048,))\n",
    "fe1 = layers.Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(512, activation = 'relu')(fe1)\n",
    "\n",
    "\n",
    "inputs2 = Input(shape = (max_length,))\n",
    "se1 = Embedding(vocab_size, emb_dim, mask_zero = True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(512)(se2)\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(512, activation = 'relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation = 'softmax')(decoder2)\n",
    "model0 = Model(inputs= [inputs1, inputs2], output= outputs)\n",
    "#plot_model(model0, to_file='plot_model/model', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 2\n",
    "\n",
    "inputs1 = Input(shape = (2048,))\n",
    "fe1 = Dense(512, activation = 'relu')(inputs1)\n",
    "fe2 = layers.Dropout(0.5)(fe1)\n",
    "\n",
    "inputs2 = Input(shape = (max_length,))\n",
    "se1 = Embedding(vocab_size, emb_dim, mask_zero = True)(inputs2)\n",
    "se2 = LSTM(512)(se1)\n",
    "decoder1 = add([fe2, se2])\n",
    "decoder2 = Dense(512, activation = 'relu')(decoder1)\n",
    "se3 = Dropout(0.5)(decoder2)\n",
    "outputs = Dense(vocab_size, activation = 'softmax')(se3)\n",
    "model1 = Model(inputs= [inputs1, inputs2], output= outputs)\n",
    "#pydot = keras.utils.vis_utils.pydot\n",
    "#plot_model(model0, to_file='plot_model/model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM #3\n",
    "\n",
    "inputs1 = Input(shape = (2048,))\n",
    "fe1 = Dense(512, activation = 'relu')(inputs1)\n",
    "fe2 = layers.Dropout(0.5)(fe1)\n",
    "\n",
    "inputs2 = Input(shape = (max_length,))\n",
    "se1 = Embedding(vocab_size, emb_dim, mask_zero = True)(inputs2)\n",
    "se2 = LSTM(512)(se1)\n",
    "decoder1 = add([fe2, se2])\n",
    "se3 = Dropout(0.5)(decoder1)\n",
    "decoder2 = Dense(512, activation = 'relu')(se3)\n",
    "\n",
    "outputs = Dense(vocab_size, activation = 'softmax')(decoder2)\n",
    "model2 = Model(inputs= [inputs1, inputs2], output= outputs)\n",
    "\n",
    "#plot_model(model0, to_file='plot_model/model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 4\n",
    "\n",
    "inputs1 = Input(shape = (2048,))\n",
    "fe1 = Dense(512, activation = 'relu')(inputs1)\n",
    "fe2 = layers.Dropout(0.5)(fe1)\n",
    "\n",
    "inputs2 = Input(shape = (max_length,))\n",
    "se1 = Embedding(vocab_size, emb_dim, mask_zero = True)(inputs2)\n",
    "se2 = LSTM(512)(se1)\n",
    "decoder1 = add([fe2, se2])\n",
    "se3 = Dropout(0.5)(decoder1)\n",
    "\n",
    "outputs = Dense(vocab_size, activation = 'softmax')(se3)\n",
    "model3 = Model(inputs= [inputs1, inputs2], output= outputs)\n",
    "\n",
    "#plot_model(model0, to_file='plot_model/model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM 1\n",
    "inputs1 = layers.Input(shape = (2048,))\n",
    "fe1 = layers.Dropout(0.5)(inputs1)\n",
    "fe2 = layers.Dense(1024, activation = 'relu')(fe1)\n",
    "\n",
    "inputs2 = layers.Input(shape = (max_length,))\n",
    "se1 = layers.Embedding(vocab_size, emb_dim)(inputs2)\n",
    "se2 = layers.Dropout(0.5)(se1)\n",
    "se3 = CuDNNLSTM(1024)(se2)\n",
    "decoder1 = layers.add([fe2, se3])\n",
    "decoder2 = layers.Dense(1024, activation = 'relu')(decoder1)\n",
    "outputs = layers.Dense(vocab_size, activation = 'softmax')(decoder2)\n",
    "model0 = Model([inputs1, inputs2], outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.layers[2].set_weights([embedding_matrix])\n",
    "model0.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.layers[3].set_weights([embedding_matrix])\n",
    "model1.layers[3].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.layers[3].set_weights([embedding_matrix])\n",
    "model2.layers[3].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.layers[3].set_weights([embedding_matrix])\n",
    "model3.layers[3].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.compile(loss = 'categorical_crossentropy', optimizer = 'adagrad', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss = 'categorical_crossentropy', optimizer = 'adagrad', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss = 'categorical_crossentropy', optimizer = 'adagrad', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(loss = 'categorical_crossentropy', optimizer = 'adagrad', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "number_photo_per_batch = 32\n",
    "steps = len(tdict)// number_photo_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epochs with batch size 32 and dense units as 256\n",
    "loss0_batch32=[]\n",
    "accuracy0_batch32 = []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=model0.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, validation_data = generator2, validation_steps = steps )\n",
    "    loss0_batch32.append(history.history['val_loss'])\n",
    "    accuracy0_batch32.append(history.history['val_accuracy'])\n",
    "    model0.save('model_weights/model' + str(j) +'.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epochs with batch size 64\n",
    "\n",
    "loss1_batch64=[]\n",
    "accuracy1_batch64 = []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=model0.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1,  validation_data = generator2, validation_steps = steps )\n",
    "    loss1_batch64.append(history.history['val_loss'])\n",
    "    accuracy1_batch64.append(history.history['val_accuracy'])\n",
    "    model0.save('model_weights_batch64/model' + str(j) +'.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs with batch size 128\n",
    "loss2_batch128=[]\n",
    "accuracy2_batch128 = []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=model0.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1,  validation_data = generator2, validation_steps = steps )\n",
    "    loss2_batch128.append(history.history['val_loss'])\n",
    "    accuracy2_batch128.append(history.history['val_accuracy'])\n",
    "    model0.save('model_weights_batch128/model' + str(j) +'.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs with batch size 256\n",
    "loss3_batch256=[]\n",
    "accuracy3_batch256 = []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=model0.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1,  validation_data = generator2, validation_steps = steps )\n",
    "    loss3_batch256.append(history.history['val_loss'])\n",
    "    accuracy3_batch256.append(history.history['val_accuracy'])\n",
    "    model0.save('model_weights_batch256/model' + str(j) +'.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with Hidden units (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots the graph of val accuracy and loss with hidden units\n",
    "plt.figure(figsize = (25,15))\n",
    "plt.plot(loss1, linewidth = 5)\n",
    "plt.plot(loss0, linewidth = 5)\n",
    "plt.plot(loss2, linewidth = 5)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Dense-256', 'Dense-512', 'Dense-1024'])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (25,15))\n",
    "plt.plot(accuracy1, linewidth = 5)\n",
    "plt.plot(accuracy0, linewidth = 5)\n",
    "plt.plot(accuracy2, linewidth = 5)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Dense-256', 'Dense-512', 'Dense-1024'])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 25})\n",
    "plt.figure(figsize=(25,15))\n",
    "plt.plot(loss0_batch32, linewidth=5)\n",
    "plt.plot(loss1_batch64, linewidth = 5)\n",
    "plt.plot(loss2_batch128, linewidth = 5, c='grey')\n",
    "plt.plot(loss3_batch256, linewidth=5)\n",
    "plt.ylabel('Validation Loss ', fontsize=25)\n",
    "plt.xlabel('Epochs', fontsize=25)\n",
    "plt.legend(['Batch-32', 'Batch-64', 'Batch-128', 'Batch-256'], loc='upper right', prop={'size':22} )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 25})\n",
    "plt.figure(figsize=(25,15))\n",
    "plt.plot(accuracy0_batch32, linewidth=5)\n",
    "plt.plot(accuracy1_batch64, linewidth=5)\n",
    "plt.plot(accuracy2_batch128, linewidth=5, c = 'grey')\n",
    "plt.plot(accuracy3_batch256, linewidth=5)\n",
    "\n",
    "plt.ylabel('Validation Accuracy', fontsize = 25)\n",
    "plt.xlabel('Epochs', fontsize=25)\n",
    "plt.legend(['Batch-32', 'Batch-64','Batch-128','Batch-256'], loc='upper left', prop = {'size': 22} )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#  Experiments with hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epochs with hidden unit 256\n",
    "\n",
    "loss0_hidden = loss0_batch\n",
    "accuracy0_hidden = accuracy0_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epochs with hidden unit 512 (LSTM) with batch size kept 32 constant\n",
    "loss0_hidden256=[]\n",
    "accuracy0_hidden256 = []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=model0.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, validation_data = generator2, validation_steps = steps )\n",
    "    loss0_hidden256.append(history.history['val_loss'])\n",
    "    accuracy0_hidden256.append(history.history['val_accuracy'])\n",
    "    model0.save('model_hidden256/model' + str(j) +'.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epochs with hidden unit 512 (LSTM) with batch size kept 32 constant\n",
    "loss1_hidden512=[]\n",
    "accuracy1_hidden512 = []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=model0.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, validation_data = generator2, validation_steps = steps )\n",
    "    loss1_hidden512.append(history.history['val_loss'])\n",
    "    accuracy1_hidden512.append(history.history['val_accuracy'])\n",
    "    model0.save('model_hidden512/model' + str(j) +'.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epochs with hidden unit 1024 (LSTM) with batch size kept 32 constant\n",
    "loss2_hidden1024=[]\n",
    "accuracy2_hidden1024 = []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=model0.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, validation_data = generator2, validation_steps = steps )\n",
    "    loss2_hidden1024.append(history.history['val_loss'])\n",
    "    accuracy2_hidden1024.append(history.history['val_accuracy'])\n",
    "    model0.save('model_hidden1024/model' + str(j) +'.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "plt.figure(figsize = (25,15))\n",
    "plt.plot(loss0_hidden256, linewidth = 5)\n",
    "plt.plot(loss1_hidden512, linewidth = 5)\n",
    "plt.plot(loss2_hidden1024, linewidth = 5)\n",
    "plt.ylabel('Validation Loss', fontsize = 25)\n",
    "plt.xlabel('Epochs', fontsize=25)\n",
    "plt.legend(['Dense-256', 'Dense-512', 'Dense-1024'], prop={'size':22})\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "plt.figure(figsize = (25,15))\n",
    "plt.plot(accuracy0_hidden256, linewidth = 5)\n",
    "plt.plot(accuracy1_hidden512, linewidth = 5)\n",
    "plt.plot(accuracy2_hidden1024, linewidth = 5)\n",
    "plt.ylabel('Validation Accuracy', fontsize = 25)\n",
    "plt.xlabel('Epochs', fontsize=25)\n",
    "plt.legend(['Dense-256', 'Dense-512', 'Dense-1024'], prop={'size':22})\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with Dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epochs with dropout layer changed from 2nd to 3rd and 6th to 9th with no dense layer removed.\n",
    "loss0_drop=[]\n",
    "accuracy0_drop = []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=model0.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, validation_data = generator2, validation_steps = steps )\n",
    "    loss0_drop.append(history.history['val_loss'])\n",
    "    accuracy0_drop.append(history.history['val_accuracy'])\n",
    "    model0.save('model_weights_drop0/model' + str(j) +'.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epochs with dropout layer changed from 2nd to 3rd and 6th to 9th with no dense layer removed.\n",
    "loss1_drop=[]\n",
    "accuracy1_drop = []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=model1.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, validation_data = generator2, validation_steps = steps )\n",
    "    loss1_drop.append(history.history['val_loss'])\n",
    "    accuracy1_drop.append(history.history['val_accuracy'])\n",
    "    model1.save('model_weights_drop1/model' + str(j) +'.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epochs with dropout layer changed from 2nd to 3rd and 6th to 9th with no dense layer removed.\n",
    "loss2_drop=[]\n",
    "accuracy2_drop = []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=model2.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, validation_data = generator2, validation_steps = steps )\n",
    "    loss2_drop.append(history.history['val_loss'])\n",
    "    accuracy2_drop.append(history.history['val_accuracy'])\n",
    "    model2.save('model_weights_drop2/model' + str(j) +'.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epochs with dropout layer changed from 2nd to 3rd and 6th to 9th with no dense layer removed.\n",
    "loss3_drop=[]\n",
    "accuracy3_drop = []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=model3.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, validation_data = generator2, validation_steps = steps )\n",
    "    loss3_drop.append(history.history['val_loss'])\n",
    "    accuracy3_drop.append(history.history['val_accuracy'])\n",
    "    model3.save('model_weights_drop3/model' + str(j) +'.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualising experiments with dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "plt.figure(figsize = (25,15))\n",
    "plt.plot(accuracy0_drop, linewidth = 5)\n",
    "plt.plot(accuracy1_drop, linewidth = 5)\n",
    "plt.plot(accuracy2_drop, linewidth = 5)\n",
    "plt.plot(accuracy3_drop, linewidth = 5)\n",
    "plt.ylabel('Validation Accuracy', fontsize = 25)\n",
    "plt.xlabel('Epochs', fontsize=25)\n",
    "plt.legend(['Dropout layer after Input-1 and Enbedding Layer', 'Dropout layer after first and second Dense layers ', \\\n",
    "            'Dropout layer after first Dense layer and after addition layer',\\\n",
    "            'Dropout layer after first Dense layer and after addition layer removing second last dense layer'],\\\n",
    "           prop={'size':20})\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "plt.figure(figsize = (25,15))\n",
    "plt.plot(loss0_drop, linewidth = 5)\n",
    "plt.plot(loss1_drop, linewidth = 5)\n",
    "plt.plot(loss2_drop, linewidth = 5)\n",
    "plt.plot(loss3_drop, linewidth = 5)\n",
    "plt.plot(loss3_drop, linewidth = 5)\n",
    "plt.ylabel('Validation Loss', fontsize = 25)\n",
    "plt.xlabel('Epochs', fontsize=25)\n",
    "plt.legend(['Dropout layer after Input-1 and Enbedding Layer', 'Dropout layer after first and second Dense layers ', \\\n",
    "            'Dropout layer after first Dense layer and after addition layer',\\\n",
    "            'Dropout layer after first Dense layer and after addition layer removing second last dense layer'],\\\n",
    "           prop={'size':20})\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.save_weights('model_weights_hidden1024/modeltrain.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.load_weights('model_weights_hidden1024/modeltrain.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = 'Images/Images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('enc_testing.pkl', 'rb') as enc_pkl:\n",
    "    enc_test = load(enc_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "def greedySearch(photo):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model0.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ixtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:]\n",
    "    final = ' '.join(final)\n",
    "    pred.append(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = {}\n",
    "for i in range(len(enc_test.keys())):\n",
    "    pic = list(enc_test.keys())[i]\n",
    "    image1 = enc_test[pic].reshape((1,2048))\n",
    "    x=plt.imread(images+pic)\n",
    "    plt.imshow(x)\n",
    "    plt.show()\n",
    "    pred1[pic] = []\n",
    "    pred1[pic].append(greedySearch(image1).split())\n",
    "    print(\"Generated Caption:\",pic,greedySearch(image1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_img = test_img[13]\n",
    "Image =open(try_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image2 = trial_vector.reshape(1,2048)\n",
    "y = plt.imread('pic_screen.png')\n",
    "plt.imshow(y)\n",
    "plt.show()\n",
    "greedySearch(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in train_features.values():\n",
    "    print(val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_trail=pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_trial_temp = {}\n",
    "\n",
    "for key in pred_trail.keys():\n",
    "    a = key.split('.')[0]\n",
    "    pred_trial_temp[a] = pred_trail[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = {}\n",
    "for keys in pred_trial_temp.keys():\n",
    "    if keys in description.keys():\n",
    "        reff = [d.split() for d in description[keys]]\n",
    "        ref[keys] = description[keys][0:5]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cap = []\n",
    "\n",
    "for k,v in pred_trial_temp.items():\n",
    "    pred_cap.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cap = [item for i in pred_cap for item in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = []\n",
    "\n",
    "for k,v in ref.items():\n",
    "    references.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = np.array(references)\n",
    "references = references[:,:, None ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in references:\n",
    "    for j in i:\n",
    "        print (j[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(pred, ref):\n",
    "    return nltk.translate.bleu_score.corpus_bleu(ref, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, i in enumerate(references):\n",
    "    print(i)\n",
    "    print(pred_cap[index])\n",
    "    print(sentence_bleu(i, pred_cap[index], weights = (1,0,0,0)))\n",
    "    print(sentence_bleu(i, pred_cap[index], weights = (0.5,0.5,0,0)))\n",
    "    print(sentence_bleu(i, pred_cap[index], weights = (0.33,0.33,0.33,0)))\n",
    "    print(sentence_bleu(i, pred_cap[index], weights = (0.25,0.25,0.25,0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno = references[0:1001]\n",
    "anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap1 = []\n",
    "for i in range(anno.shape[0]):\n",
    "    c = []\n",
    "    for j in range(anno.shape[1]):\n",
    "        line = anno[i][j]\n",
    "        linearr = line[0].split(' ')\n",
    "        c.append(linearr)\n",
    "    cap1.append(c)\n",
    "print(cap1)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pred_cap[0:1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=[]\n",
    "\n",
    "for i in p:\n",
    "    print(i)\n",
    "    q.append(i[0:11])\n",
    "    #q.append(i.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XCEPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Xception(weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xception = Model(model3.input, model3.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector2(img):\n",
    "    img = preprocess(img)\n",
    "    featvec = xception.predict(img)\n",
    "    featvec = np.reshape(featvec, featvec.shape[1])\n",
    "    return featvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding all train images\n",
    "\n",
    "check = time()\n",
    "enc_train2 = {}\n",
    "\n",
    "for img in train_img:\n",
    "    enc_train2[img[len(images):]] = vector2(img)\n",
    "print('time taken in sec', time()-check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the features\n",
    "with open('enc_trainimg2.pkl', 'wb') as enc_pkl2:\n",
    "    pickle.dump(enc_train2, enc_pkl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = time()\n",
    "enc_val2 = {}\n",
    "\n",
    "for img in val_img:\n",
    "    enc_val2[img[len(images):]] = vector2(img)\n",
    "print('time taken in sec', time()-check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the features\n",
    "with open('enc_valimg2.pkl', 'wb') as enc_valpkl2:\n",
    "    pickle.dump(enc_val2, enc_valpkl2)\n",
    "val_features2 = load(open('enc_valimg2.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding all test images\n",
    "check = time()\n",
    "enc_test2 = dict()\n",
    "\n",
    "for img in test_img:\n",
    "    enc_test2[img[len(images):]] = vector2(img)\n",
    "print('time taken in sec', time()-check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving test feature\n",
    "\n",
    "with open('enc_testing2.pkl', 'wb') as enc_pickle2:\n",
    "    pickle.dump(enc_test2, enc_pickle2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features2 = load(open('enc_trainimg2.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "inp1 = Input(shape = (2048,))\n",
    "ffee1 = Dropout(0.5)(inp1)\n",
    "ffee2 = Dense(512, activation = 'relu')(ffee1)\n",
    "\n",
    "inp2 = Input(shape = (max_length,))\n",
    "ssee1 = Embedding(vocab_size, emb_dim, mask_zero = True)(inp2)\n",
    "ssee2 = Dropout(0.5)(ssee1)\n",
    "ssee3 = LSTM(512)(ssee2)\n",
    "dcd1 = add([ffee2, ssee3])\n",
    "dcd2 = Dense(512, activation = 'relu')(dcd1)\n",
    "opt = Dense(vocab_size, activation = 'softmax')(dcd2)\n",
    "modelxception = Model(inputs= [inp1, inp2], output= opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelxception.layers[2].set_weights([embedding_matrix])\n",
    "modelxception.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lr = 0.001\n",
    "def schedule(epoch):\n",
    "    return start_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.LearningRateScheduler(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelxception.compile(loss = 'categorical_crossentropy', optimizer = 'adagrad', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "number_photo_per_batch = 32\n",
    "steps = len(tdict)// number_photo_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epochs with dropout layer changed from 2nd to 3rd and 6th to 9th with no dense layer removed.\n",
    "loss0_xception=[]\n",
    "accuracy0_xception= []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features2, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features2, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelxception.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, validation_data = generator2, validation_steps = steps )\n",
    "    loss0_xception.append(history.history['val_loss'])\n",
    "    accuracy0_xception.append(history.history['val_accuracy'])\n",
    "    modelxception.save('model_weights2/model' + str(j) +'.h5' )\n",
    "round(modelxception.optimizer.lr.numpy(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(epochs):\n",
    "    generator2 = data_generator(tdict, train_features2, wordtoix, max_length, number_photo_per_batch)\n",
    "    model1.fit_generator(generator2, epochs = 1, steps_per_epoch= steps ,verbose = 1 )\n",
    "    model1.save('model_weights2/model' + str(j) +'.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(epochs):\n",
    "    generator2 = data_generator(tdict, train_features2, wordtoix, max_length, number_photo_per_batch)\n",
    "    model1.fit_generator(generator2, epochs = 1, steps_per_epoch= steps ,verbose = 1 )\n",
    "    model1.save('model_weights2/model' + str(j) +'.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for j in range(epochs):\n",
    "    generator2 = data_generator(tdict, train_features2, wordtoix, max_length, number_photo_per_batch)\n",
    "    model.fit_generator(generator2, epochs = 1, steps_per_epoch= steps ,verbose = 1 )\n",
    "    model.save('model_weights2/model' + str(j) +'.h5' )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelxception.save_weights('model_weights2/modeltrain.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelxception.load_weights('model_weights2/modeltrain.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('enc_testing2.pkl', 'rb') as enc_pkl:\n",
    "    enc_test2 = load(enc_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = []\n",
    "def greedySearch(photo):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
    "        photo = photo.reshape(1,2048)\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length).reshape(1, max_length)\n",
    "        yhat = modelxception.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ixtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:]\n",
    "    final = ' '.join(final)\n",
    "    pred3.append(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred4 = {}\n",
    "for i in range(len(enc_test2.keys())):\n",
    "    pic = list(enc_test2.keys())[i]\n",
    "    image1 = enc_test2[pic].reshape((1,2048))\n",
    "    x=plt.imread(images+pic) \n",
    "    plt.imshow(x)\n",
    "    plt.show()\n",
    "    pred4[pic] = []\n",
    "    pred4[pic].append(greedySearch(image1).split())\n",
    "    print(\"Generated Caption:\",greedySearch(image1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_trail2=pred4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_trail_temp2 = {}\n",
    "for keys in pred_trail2:\n",
    "    key = keys.split('.')[0]\n",
    "    pred_trail_temp2[key] = pred_trail2[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref2 = {}\n",
    "for keys in pred_trail_temp2.keys():\n",
    "    if keys in description.keys():\n",
    "        reff2 = [d.split() for d in description[keys]]\n",
    "        ref2[keys] = description[keys][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references2 = []\n",
    "\n",
    "for k,v in ref2.items():\n",
    "    references2.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references2 = np.array(references2)\n",
    "references2 = references2[:,:, None ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap2 = []\n",
    "\n",
    "for i in range(references2.shape[0]):\n",
    "    c = []\n",
    "    for j in range(references2.shape[1]):\n",
    "        line = references2[i][j]\n",
    "        linearr = line[0].split(' ')\n",
    "        c.append(linearr)\n",
    "    cap2.append(c)\n",
    "print(cap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predtt = pred3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predtt = set(predtt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst2=[]\n",
    "for v in pred4.values():\n",
    "    lst2.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst2=[items for i in lst2 for items in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INCEPTION RESNET V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = InceptionResNetV2(weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionresnet = Model(model4.input, model4.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector3(img):\n",
    "    img = preprocess(img)\n",
    "    featvec = inceptionresnet.predict(img)\n",
    "    featvec = np.reshape(featvec, featvec.shape[1])\n",
    "    return featvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding all train images\n",
    "\n",
    "check = time()\n",
    "enc_train3 = {}\n",
    "\n",
    "for img in train_img:\n",
    "    enc_train3[img[len(images):]] = vector3(img)\n",
    "print('time taken in sec', time()-check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the features\n",
    "with open('enc_trainimg3.pkl', 'wb') as enc_pkl3:\n",
    "    pickle.dump(enc_train3, enc_pkl3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = time()\n",
    "enc_val3 = {}\n",
    "\n",
    "for img in val_img:\n",
    "    enc_val3[img[len(images):]] = vector3(img)\n",
    "print('time taken in sec', time()-check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the features\n",
    "with open('enc_valimg3.pkl', 'wb') as enc_valpkl3:\n",
    "    pickle.dump(enc_val3, enc_valpkl3)\n",
    "val_features3 = load(open('enc_valimg3.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding all test images\n",
    "check = time()\n",
    "enc_test3 = dict()\n",
    "\n",
    "for img in test_img:\n",
    "    enc_test3[img[len(images):]] = vector3(img)\n",
    "print('time taken in sec', time()-check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving test feature\n",
    "\n",
    "with open('enc_testing3.pkl', 'wb') as enc_pickle3:\n",
    "    pickle.dump(enc_test3, enc_pickle3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features3 = load(open('enc_trainimg3.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in train_features3.values():\n",
    "    print(val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "input1 = Input(shape = (1536,))\n",
    "fee1 = Dropout(0.5)(input1)\n",
    "fee2 = Dense(512, activation = 'relu')(fee1)\n",
    "\n",
    "input2 = Input(shape = (max_length,))\n",
    "see1 = Embedding(vocab_size, emb_dim, mask_zero = True)(input2)\n",
    "see2 = Dropout(0.5)(see1)\n",
    "see3 = LSTM(512)(see2)\n",
    "decoders1 = add([fee2, see3])\n",
    "decoders2 = Dense(512, activation = 'relu')(decoders1)\n",
    "output = Dense(vocab_size, activation = 'softmax')(decoders2)\n",
    "modelincpres = Model(inputs= [input1, input2], output= output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelincpres.layers[2].set_weights([embedding_matrix])\n",
    "modelincpres.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelincpres.compile(loss = 'categorical_crossentropy', optimizer = 'adagrad', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "number_photo_per_batch = 32\n",
    "steps = len(tdict)// number_photo_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss0_irv2=[]\n",
    "accuracy0_irv2= []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features3, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features3, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelincpres.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, validation_data = generator2, validation_steps = steps )\n",
    "    loss0_irv2.append(history.history['val_loss'])\n",
    "    accuracy0_irv2.append(history.history['val_accuracy'])\n",
    "    modelincpres.save('model_weights3/model' + str(j) +'.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(epochs):\n",
    "    generator3 = data_generator(tdict, train_features3, wordtoix, max_length, number_photo_per_batch)\n",
    "    model2.fit_generator(generator3, epochs = 1, steps_per_epoch= steps ,verbose = 1 )\n",
    "    model2.save('model_weights3/model' + str(j) +'.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(epochs):\n",
    "    generator3 = data_generator(tdict, train_features3, wordtoix, max_length, number_photo_per_batch)\n",
    "    model2.fit_generator(generator3, epochs = 1, steps_per_epoch= steps ,verbose = 1 )\n",
    "    model2.save('model_weights3/model' + str(j) +'.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelincpres.save_weights('model_weights2/modeltrain.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelincpres.load_weights('model_weights2/modeltrain.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('enc_testing3.pkl', 'rb') as enc_pkl:\n",
    "    enc_test3 = load(enc_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = []\n",
    "def greedySearch3(photo):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
    "        photo = photo.reshape(1, 1536)\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length).reshape(1, max_length)\n",
    "        yhat = modelincpres.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ixtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:]\n",
    "    final = ' '.join(final)\n",
    "    return final\n",
    "    pred3.append(final)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred4 = {}\n",
    "for i in range(len(enc_test3.keys())):\n",
    "    pic = list(enc_test3.keys())[i]\n",
    "    image1 = enc_test3[pic].reshape((1,1536))\n",
    "for i in range(len(enc_test3.keys())):\n",
    "    pic = list(enc_test3.keys())[i]\n",
    "    image1 = enc_test3[pic]\n",
    "    x=plt.imread(images+pic) \n",
    "    plt.imshow(x)\n",
    "    plt.show()\n",
    "    pred4[pic] = []\n",
    "    pred4[pic].append(greedySearch3(image1).split())\n",
    "    print(\"Generated Caption:\",greedySearch3(image1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASNET LARGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet = nasnet.NASNetLarge(weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnl = Model(nnet.input, nnet.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess2(img):\n",
    "    all_images = image.load_img(img, target_size=(331,331))\n",
    "    x_axis = image.img_to_array(all_images)\n",
    "    print(x_axis.shape)\n",
    "    x_axis = np.expand_dims(x_axis, axis = 0)\n",
    "    print(x_axis.shape)\n",
    "    x_axis = preprocess_input(x_axis)\n",
    "    return x_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir('Images/Images'):\n",
    "    all_images = image.load_img(f'Images/Images/{f}', target_size=(331,331))\n",
    "    x_axis = image.img_to_array(all_images)\n",
    "    print(x_axis.shape)\n",
    "    x_axis = np.expand_dims(x_axis, axis = 0)\n",
    "    print(x_axis.shape)\n",
    "    x_axis = preprocess_input(x_axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector4(img):\n",
    "    img = preprocess2(img)\n",
    "    featvec = nnl.predict(img)\n",
    "    featvec = np.reshape(featvec, featvec.shape[1])\n",
    "    return featvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding all train images\n",
    "\n",
    "check = time()\n",
    "enc_train4 = {}\n",
    "\n",
    "for img in train_img:\n",
    "    enc_train4[img[len(images):]] = vector4(img)\n",
    "print('time taken in sec', time()-check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the features\n",
    "with open('enc_trainimg4.pkl', 'wb') as enc_pkl4:\n",
    "    pickle.dump(enc_train4, enc_pkl4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = time()\n",
    "enc_val4 = {}\n",
    "\n",
    "for img in val_img:\n",
    "    enc_val4[img[len(images):]] = vector4(img)\n",
    "print('time taken in sec', time()-check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the features\n",
    "with open('enc_valimg4.pkl', 'wb') as enc_valpkl4:\n",
    "    pickle.dump(enc_val4, enc_valpkl4)\n",
    "val_features4 = load(open('enc_valimg4.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding all test images\n",
    "check = time()\n",
    "enc_test4 = dict()\n",
    "\n",
    "for img in test_img:\n",
    "    enc_test4[img[len(images):]] = vector4(img)\n",
    "print('time taken in sec', time()-check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving test feature\n",
    "\n",
    "with open('enc_testing4.pkl', 'wb') as enc_pickle4:\n",
    "    pickle.dump(enc_test4, enc_pickle4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features4 = load(open('enc_trainimg4.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "inputt1 = Input(shape = (4032,))\n",
    "ffe1 = Dropout(0.5)(inputt1)\n",
    "ffe2 = Dense(1024, activation = 'relu')(ffe1)\n",
    "\n",
    "inputt2 = Input(shape = (max_length,))\n",
    "sse1 = Embedding(vocab_size, emb_dim, mask_zero = True)(inputt2)\n",
    "sse2 = Dropout(0.5)(sse1)\n",
    "sse3 = LSTM(1024)(sse2)\n",
    "decoderss1 = add([ffe2, sse3])\n",
    "decoderss2 = Dense(1024, activation = 'relu')(decoderss1)\n",
    "outputt = Dense(vocab_size, activation = 'softmax')(decoderss2)\n",
    "modelnnl = Model(inputs= [inputt1, inputt2], output= outputt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnnl.layers[2].set_weights([embedding_matrix])\n",
    "modelnnl.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch<5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr*tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback2 = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "earlystopping = EarlyStopping(monitor = 'val_loss', mode = 'auto', min_delta = 0, patience = 1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer0 = keras.optimizers.adagrad(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = keras.optimizers.adagrad(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = keras.optimizers.adagrad(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnnl.compile(loss = 'categorical_crossentropy', optimizer = optimizer0, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnnl.compile(loss = 'categorical_crossentropy', optimizer = optimizer1, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnnl.compile(loss = 'categorical_crossentropy', optimizer = optimizer2, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnnl.compile(loss = 'categorical_crossentropy', optimizer = 'adagrad', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "number_photo_per_batch = 32\n",
    "steps = len(tdict)// number_photo_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adagrad\n",
    "loss0_nnl_adagrad=[]\n",
    "accuracy0_nnl_adagrad= []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelnnl.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, validation_data = generator2, validation_steps = steps )\n",
    "    loss0_nnl_adagrad.append(history.history['val_loss'])\n",
    "    accuracy0_nnl_adagrad.append(history.history['val_accuracy'])\n",
    "    modelnnl.save('model_weights4/model' + str(j) +'.h5' )\n",
    "print(modelnnl.optimizer.lr.numpy(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adagrad with batch 1024 \n",
    "loss0_nnl_adagrad=[]\n",
    "accuracy0_nnl_adagrad= []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelnnl.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, validation_data = generator2, validation_steps = steps )\n",
    "    loss0_nnl_adagrad.append(history.history['val_loss'])\n",
    "    accuracy0_nnl_adagrad.append(history.history['val_accuracy'])\n",
    "    modelnnl.save('model_weights4/model' + str(j) +'.h5' )\n",
    "print(modelnnl.optimizer.lr.numpy(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adagrad with learning rate 0.1 \n",
    "loss0_op0_adagrad=[]\n",
    "accuracy0_op0_adagrad= []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelnnl.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, validation_data = generator2, validation_steps = steps )\n",
    "    loss0_op0_adagrad.append(history.history['val_loss'])\n",
    "    accuracy0_op0_adagrad.append(history.history['val_accuracy'])\n",
    "    modelnnl.save('model_weights4/model' + str(j) +'.h5' )\n",
    "print(modelnnl.optimizer.lr.numpy(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adagrad with learning rate 0.001 \n",
    "loss0_op1_adagrad=[]\n",
    "accuracy0_op1_adagrad= []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelnnl.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, validation_data = generator2, validation_steps = steps )\n",
    "    loss0_op1_adagrad.append(history.history['val_loss'])\n",
    "    accuracy0_op1_adagrad.append(history.history['val_accuracy'])\n",
    "    modelnnl.save('model_weights4/model' + str(j) +'.h5' )\n",
    "print(modelnnl.optimizer.lr.numpy(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adagrad with learning rate 0.001 including callback\n",
    "loss0_op1c_adagrad=[]\n",
    "accuracy0_op1c_adagrad= []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelnnl.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1,callbacks=[callback2], validation_data = generator2, validation_steps = steps )\n",
    "    loss0_op1c_adagrad.append(history.history['val_loss'])\n",
    "    accuracy0_op1c_adagrad.append(history.history['val_accuracy'])\n",
    "    modelnnl.save('model_weights4/model' + str(j) +'.h5' )\n",
    "print(modelnnl.optimizer.lr.numpy(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adagrad with learning rate 0.0001 \n",
    "loss0_op2_adagrad=[]\n",
    "accuracy0_op2_adagrad= []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelnnl.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, validation_data = generator2, validation_steps = steps )\n",
    "    loss0_op2_adagrad.append(history.history['val_loss'])\n",
    "    accuracy0_op2_adagrad.append(history.history['val_accuracy'])\n",
    "    modelnnl.save('model_weights4/model' + str(j) +'.h5' )\n",
    "print(modelnnl.optimizer.lr.numpy(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adagrad with earlystopping\n",
    "loss0_nnl_adagrades=[]\n",
    "accuracy0_nnl_adagrades= []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelnnl.fit_generator(generator, epochs = 20, steps_per_epoch= steps ,verbose = 1, callbacks= [callback2, earlystopping], validation_data = generator2, validation_steps = steps )\n",
    "    loss0_nnl_adagrades.append(history.history['val_loss'])\n",
    "    accuracy0_nnl_adagrades.append(history.history['val_accuracy'])\n",
    "    modelnnl.save('model_weights4/model' + str(j) +'.h5' )\n",
    "print(modelnnl.optimizer.lr.numpy(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy0_nnl_adagrades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD\n",
    "loss0_nnl_sgd=[]\n",
    "accuracy0_nnl_sgd= []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelnnl.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, callbacks=[callback2], validation_data = generator2, validation_steps = steps )\n",
    "    loss0_nnl_sgd.append(history.history['val_loss'])\n",
    "    accuracy0_nnl_sgd.append(history.history['val_accuracy'])\n",
    "    modelnnl.save('model_weights_nnlsgd/model' + str(j) +'.h5' )\n",
    "print(modelnnl.optimizer.lr.numpy(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD\n",
    "loss0_nnl_sgdes=[]\n",
    "accuracy0_nnl_sgdes= []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelnnl.fit_generator(generator, epochs = 20, steps_per_epoch= steps ,verbose = 1, callbacks=[callback2,earlystopping], validation_data = generator2, validation_steps = steps )\n",
    "    loss0_nnl_sgdes.append(history.history['val_loss'])\n",
    "    accuracy0_nnl_sgdes.append(history.history['val_accuracy'])\n",
    "    modelnnl.save('model_weights_nnlsgd/model' + str(j) +'.h5' )\n",
    "print(modelnnl.optimizer.lr.numpy(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adam\n",
    "loss0_nnl_adam=[]\n",
    "accuracy0_nnl_adam= []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelnnl.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, callbacks=[callback2], validation_data = generator2, validation_steps = steps )\n",
    "    loss0_nnl_adam.append(history.history['val_loss'])\n",
    "    accuracy0_nnl_adam.append(history.history['val_accuracy'])\n",
    "    modelnnl.save('model_weights_nnladam/model' + str(j) +'.h5' )\n",
    "print(modelnnl.optimizer.lr.numpy(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adam with earlystopping\n",
    "loss0_nnl_adames=[]\n",
    "accuracy0_nnl_adames= []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelnnl.fit_generator(generator, epochs = 20, steps_per_epoch= steps ,verbose = 1, callbacks=[callback2, earlystopping], validation_data = generator2, validation_steps = steps )\n",
    "    loss0_nnl_adames.append(history.history['val_loss'])\n",
    "    accuracy0_nnl_adames.append(history.history['val_accuracy'])\n",
    "    modelnnl.save('model_weights_nnladam/model' + str(j) +'.h5' )\n",
    "print(modelnnl.optimizer.lr.numpy(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adadelta\n",
    "loss0_nnl_adadelta=[]\n",
    "accuracy0_nnl_adadelta= []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelnnl.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, callbacks=[callback2], validation_data = generator2, validation_steps = steps )\n",
    "    loss0_nnl_adadelta.append(history.history['val_loss'])\n",
    "    accuracy0_nnl_adadelta.append(history.history['val_accuracy'])\n",
    "    modelnnl.save('model_weights_nnladadelta/model' + str(j) +'.h5' )\n",
    "print(modelnnl.optimizer.lr.numpy(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adadelta with earlystopping\n",
    "loss0_nnl_adadeltaes=[]\n",
    "accuracy0_nnl_adadeltaes= []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelnnl.fit_generator(generator, epochs = 20, steps_per_epoch= steps ,verbose = 1, callbacks=[callback2, earlystopping], validation_data = generator2, validation_steps = steps )\n",
    "    loss0_nnl_adadeltaes.append(history.history['val_loss'])\n",
    "    accuracy0_nnl_adadeltaes.append(history.history['val_accuracy'])\n",
    "    modelnnl.save('model_weights_nnladadelta/model' + str(j) +'.h5' )\n",
    "print(modelnnl.optimizer.lr.numpy(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adamx\n",
    "loss0_nnl_adamax=[]\n",
    "accuracy0_nnl_adamax= []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelnnl.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, callbacks=[callback2], validation_data = generator2, validation_steps = steps )\n",
    "    loss0_nnl_adamax.append(history.history['val_loss'])\n",
    "    accuracy0_nnl_adamax.append(history.history['val_accuracy'])\n",
    "    modelnnl.save('model_weights_nnladamax/model' + str(j) +'.h5' )\n",
    "print(modelnnl.optimizer.lr.numpy(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adamx with earlystopping\n",
    "loss0_nnl_adamaxes=[]\n",
    "accuracy0_nnl_adamaxes= []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelnnl.fit_generator(generator, epochs = 20, steps_per_epoch= steps ,verbose = 1, callbacks=[callback2, earlystopping], validation_data = generator2, validation_steps = steps )\n",
    "    loss0_nnl_adamaxes.append(history.history['val_loss'])\n",
    "    accuracy0_nnl_adamaxes.append(history.history['val_accuracy'])\n",
    "    modelnnl.save('model_weights_nnladamax/model' + str(j) +'.h5' )\n",
    "print(modelnnl.optimizer.lr.numpy(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nadam\n",
    "loss0_nnl_nadam=[]\n",
    "accuracy0_nnl_nadam= []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelnnl.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, callbacks=[callback2], validation_data = generator2, validation_steps = steps )\n",
    "    loss0_nnl_nadam.append(history.history['val_loss'])\n",
    "    accuracy0_nnl_nadam.append(history.history['val_accuracy'])\n",
    "    modelnnl.save('model_weights_nnlnadam/model' + str(j) +'.h5' )\n",
    "print(modelnnl.optimizer.lr.numpy(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nadam with earlystopping\n",
    "loss0_nnl_nadames=[]\n",
    "accuracy0_nnl_nadames= []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelnnl.fit_generator(generator, epochs = 20, steps_per_epoch= steps ,verbose = 1, callbacks=[callback2, earlystopping], validation_data = generator2, validation_steps = steps )\n",
    "    loss0_nnl_nadames.append(history.history['val_loss'])\n",
    "    accuracy0_nnl_nadames.append(history.history['val_accuracy'])\n",
    "    modelnnl.save('model_weights_nnlnadam/model' + str(j) +'.h5' )\n",
    "print(modelnnl.optimizer.lr.numpy(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmsprop\n",
    "loss0_nnl_rmsprop =[]\n",
    "accuracy0_nnl_rmsprop = []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelnnl.fit_generator(generator, epochs = 1, steps_per_epoch= steps ,verbose = 1, callbacks=[callback2], validation_data = generator2, validation_steps = steps )\n",
    "    loss0_nnl_rmsprop.append(history.history['val_loss'])\n",
    "    accuracy0_nnl_rmsprop.append(history.history['val_accuracy'])\n",
    "    modelnnl.save('model_weights_nnlrmsprop/model' + str(j) +'.h5' )\n",
    "lrr = modelnnl.optimizer.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmsprop with earlystopping\n",
    "loss0_nnl_rmspropes =[]\n",
    "accuracy0_nnl_rmspropes = []\n",
    "for j in range(epochs):\n",
    "    generator = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    generator2 = data_generator(valdict, val_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    history=modelnnl.fit_generator(generator, epochs = 20, steps_per_epoch= steps ,verbose = 1, callbacks=[callback2, earlystopping], validation_data = generator2, validation_steps = steps )\n",
    "    loss0_nnl_rmspropes.append(history.history['val_loss'])\n",
    "    accuracy0_nnl_rmspropes.append(history.history['val_accuracy'])\n",
    "    modelnnl.save('model_weights_nnlrmsprop/model' + str(j) +'.h5' )\n",
    "lrr = modelnnl.optimizer.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(epochs):\n",
    "    generator4 = data_generator(tdict, train_features4, wordtoix, max_length, number_photo_per_batch)\n",
    "    model3.fit_generator(generator4, epochs = 1, steps_per_epoch= steps ,verbose = 1 )\n",
    "    model3.save('model_weights4/model' + str(j) +'.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss0_inception = loss0_drop\n",
    "accuracy0_inception = accuracy0_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "plt.figure(figsize = (25,15))\n",
    "plt.plot(accuracy0_inception, linewidth = 5)\n",
    "plt.plot(accuracy0_xception, linewidth = 5)\n",
    "plt.plot(accuracy0_irv2, linewidth = 5)\n",
    "plt.plot(accuracy0_nnl, linewidth = 5)\n",
    "plt.ylabel('Validation Accuracy', fontsize = 25)\n",
    "plt.xlabel('Epochs', fontsize=25)\n",
    "plt.legend(['Inception V3', 'Xception', \\\n",
    "            'Inception ResNet V2',\\\n",
    "            'NASNetLarge'],\\\n",
    "           prop={'size':20})\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "plt.figure(figsize = (25,15))\n",
    "plt.plot(loss0_inception, linewidth = 5)\n",
    "plt.plot(loss0_xception, linewidth = 5)\n",
    "plt.plot(loss0_irv2, linewidth = 5)\n",
    "plt.plot(loss0_nnl, linewidth = 5)\n",
    "plt.ylabel('Validation Loss', fontsize = 25)\n",
    "plt.xlabel('Epochs', fontsize=25)\n",
    "plt.legend(['Inception V3', 'Xception', \\\n",
    "            'Inception ResNet V2',\\\n",
    "            'NASNetLarge'],\\\n",
    "           prop={'size':20})\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss0_nnl_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization with different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "plt.figure(figsize = (25,15))\n",
    "plt.plot(loss0_nnl_sgd, linewidth = 5)\n",
    "plt.plot(loss0_nnl_adam, linewidth = 5)\n",
    "plt.plot(loss0_nnl_nadam, linewidth = 5)\n",
    "plt.plot(loss0_nnl_adamax, linewidth = 5)\n",
    "plt.plot(loss0_nnl_adadelta, linewidth = 5)\n",
    "plt.plot(loss0_nnl_rmsprop, linewidth = 5)\n",
    "plt.plot(loss0_nnl_adagrad, linewidth = 5)\n",
    "plt.ylabel('Validation Loss', fontsize = 25)\n",
    "plt.xlabel('Epochs', fontsize=25)\n",
    "plt.legend(['SGD- lr- 0.01', 'ADAM- lr- 0.001', \\\n",
    "            'NADAM -lr- 0.001',\\\n",
    "            'ADAMAX- lr- 0.002', 'ADADELTA- lr- 1.0',\\\n",
    "            'RMSPROP- lr- 0.001', 'Adagrad- lr- 0.01'],\\\n",
    "           prop={'size':20})\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "plt.figure(figsize = (25,15))\n",
    "plt.plot(accuracy0_nnl_sgd, linewidth = 5)\n",
    "plt.plot(accuracy0_nnl_adam, linewidth = 5)\n",
    "plt.plot(accuracy0_nnl_nadam, linewidth = 5)\n",
    "plt.plot(accuracy0_nnl_adamax, linewidth = 5)\n",
    "plt.plot(accuracy0_nnl_adadelta, linewidth = 5)\n",
    "plt.plot(accuracy0_nnl_rmsprop, linewidth = 5)\n",
    "plt.plot(accuracy0_nnl_adagrad, linewidth = 5)\n",
    "plt.ylabel('Validation Accuracy', fontsize = 25)\n",
    "plt.xlabel('Epochs', fontsize=25)\n",
    "plt.legend(['SGD- lr- 0.01', 'ADAM- lr- 0.001', \\\n",
    "            'NADAM -lr- 0.001',\\\n",
    "            'ADAMAX- lr- 0.002', 'ADADELTA- lr- 1.0',\\\n",
    "            'RMSPROP- lr- 0.001', 'Adagrad- lr- 0.01'],\\\n",
    "           prop={'size':20})\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "plt.figure(figsize = (25,15))\n",
    "plt.plot(loss0_op0_adagrad, linewidth = 5)\n",
    "plt.plot(loss0_nnl_adagrad, linewidth = 5)\n",
    "plt.plot(loss0_op1_adagrad, linewidth = 5)\n",
    "plt.plot(loss0_op2_adagrad, linewidth = 5)\n",
    "plt.ylabel('Validation Loss', fontsize = 25)\n",
    "plt.xlabel('Epochs', fontsize=25)\n",
    "plt.legend(['AdaGrad lr-0.1', 'AdaGrad lr-0.01' ,'AdaGrad lr-0.001', \\\n",
    "            'AdaGrad lr-0.0001'],\\\n",
    "           prop={'size':20})\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "plt.figure(figsize = (25,15))\n",
    "plt.plot(accuracy0_op0_adagrad, linewidth = 5)\n",
    "plt.plot(accuracy0_nnl_adagrad, linewidth = 5)\n",
    "plt.plot(accuracy0_op1_adagrad, linewidth = 5)\n",
    "plt.plot(accuracy0_op2_adagrad, linewidth = 5)\n",
    "plt.ylabel('Validation Accuracy', fontsize = 25)\n",
    "plt.xlabel('Epochs', fontsize=25)\n",
    "plt.legend(['AdaGrad lr-0.1', 'AdaGrad lr-0.01' ,'AdaGrad lr-0.001', \\\n",
    "            'AdaGrad lr-0.0001'],\\\n",
    "           prop={'size':20})\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "plt.figure(figsize = (25,15))\n",
    "\n",
    "plt.plot(loss0_nnl_adagrad, linewidth = 5)\n",
    "plt.plot(loss0_op1c_adagrad, linewidth = 5)\n",
    "\n",
    "plt.plot(loss0_op1_adagrad, linewidth = 5)\n",
    "\n",
    "plt.ylabel('Validation Loss', fontsize = 25)\n",
    "plt.xlabel('Epochs', fontsize=25)\n",
    "plt.legend(['AdaGrad','AdaGrad lr-0.001 with learning rate scheduler', 'AdaGrad lr-0.001'],\\\n",
    "           prop={'size':20})\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "plt.figure(figsize = (25,15))\n",
    "\n",
    "plt.plot(accuracy0_nnl_adagrad, linewidth = 5)\n",
    "plt.plot(accuracy0_op1c_adagrad, linewidth = 5)\n",
    "\n",
    "plt.plot(accuracy0_op1_adagrad, linewidth = 5)\n",
    "\n",
    "plt.ylabel('Validation Accuracy', fontsize = 25)\n",
    "plt.xlabel('Epochs', fontsize=25)\n",
    "plt.legend(['AdaGrad','AdaGrad lr-0.001 with learning rate scheduler', 'AdaGrad lr-0.001'],\\\n",
    "           prop={'size':20})\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnnl.save_weights('model_weights4/modeltrain.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnnl.load_weights('model_weights4/modeltrain.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('enc_testing4.pkl', 'rb') as enc_pickle4:\n",
    "    enc_test4 = load(enc_pickle4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred5 = []\n",
    "def greedySearch(photo):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
    "        photo = photo.reshape(1,4032)\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length).reshape(1, max_length)\n",
    "        yhat = modelnnl.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ixtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:]\n",
    "    final = ' '.join(final)\n",
    "    pred5.append(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred4 = {}\n",
    "for i in range(len(enc_test4.keys())):\n",
    "    pic = list(enc_test4.keys())[i]\n",
    "    image1 = enc_test4[pic].reshape((1,4032))\n",
    "    x=plt.imread(images+pic) \n",
    "    plt.imshow(x)\n",
    "    plt.show()\n",
    "    pred4[pic] = []\n",
    "    pred4[pic].append(greedySearch(image1).split())\n",
    "    print(\"Generated Caption:\",greedySearch(image1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_trial4 = pred4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_trial_temp4 = {}\n",
    "for keys in pred_trial4:\n",
    "    key = keys.split('.')[0]\n",
    "    pred_trial_temp4[key] = pred_trial4[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref4 = {}\n",
    "for keys in pred_trial_temp4.keys():\n",
    "    if keys in description.keys():\n",
    "        reff = [d.split() for d in description[keys]]\n",
    "        ref4[keys] = description[keys][0:5]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cap4 = []\n",
    "\n",
    "for k,v in pred_trial_temp4.items():\n",
    "    pred_cap4.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cap4 = [item for i in pred_cap4 for item in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "references4 = []\n",
    "\n",
    "for k,v in ref4.items():\n",
    "    references4.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references4 = np.array(references4)\n",
    "references4 = references4[:,:, None ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno = references4[0:1001]\n",
    "anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap4 = []\n",
    "for i in range(anno.shape[0]):\n",
    "    c = []\n",
    "    for j in range(anno.shape[1]):\n",
    "        line = anno[i][j]\n",
    "        linearr = line[0].split(' ')\n",
    "        c.append(linearr)\n",
    "    cap4.append(c)\n",
    "print(cap4)\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap1024 = []\n",
    "for i in range(anno.shape[0]):\n",
    "    c = []\n",
    "    for j in range(anno.shape[1]):\n",
    "        line = anno[i][j]\n",
    "        linearr = line[0].split(' ')\n",
    "        c.append(linearr)\n",
    "    cap1024.append(c)\n",
    "print(cap1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_captions = pred_cap4[0:1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions=[]\n",
    "\n",
    "for i in p:\n",
    "    captions.append(i[0:7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU Score batch hidden unit size 512\n",
    "b1512#(corpus_bleu(cap4, q, weights=(1,0,0,0)))\n",
    "b2512#(corpus_bleu(cap4, q, weights=(0.5,0.5,0,0)))\n",
    "b3512#(corpus_bleu(cap4, q, weights=(0.3,0.3,0.3,0)))\n",
    "b4512#(corpus_bleu(cap4, q, weights=(0.25,0.25,0.25,0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU Score batch hidden unit size 1024\n",
    "b11024=(corpus_bleu(cap4, q, weights=(1,0,0,0)))\n",
    "b21024=(corpus_bleu(cap4, q, weights=(0.5,0.5,0,0)))\n",
    "b31024=(corpus_bleu(cap4, q, weights=(0.3,0.3,0.3,0)))\n",
    "b41024=(corpus_bleu(cap4, q, weights=(0.25,0.25,0.25,0.25)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
